### 前言

​	这是第一版，完成于 2025.4.30。

​	看到 tuya 开发者群里的机器人，心血来潮也想做个有 AI Agent 能力的微信机器人，结果只做成单一的微信机器人 AI 自动回复消息。至于只用本地大模型第一是因为 api 要钱不想花钱，第二是，当最强的大模型在和博士 pk 时，其他大模型可以用得很好了，完全够普通人用。他们在大模型混战是因为要争 AI 高地，而普通人能用，达到普通人预期要求就行。

<img src="./md-pictures/tuya小助理对话.png#pic" alt="tuya" style="zoom:50%;" />

**参考**：

​	wxauto 的 github 项目 [cluic/wxauto: Windows版本微信客户端（非网页版）自动化，可实现简单的发送、接收微信消息，简单微信机器人](https://github.com/cluic/wxauto)

**功能**：

​	监听微信用户，调用本地 ollama 大模型，使用AI逐句回复消息，能识别文字和表情包（deepseek除外）。

**技术栈**：

- wxauto
- ollama
- python
- qwen3
- curosr

### 部署：

1、安装 ollama 后，下载大模型

```cmd
ollama run qwen3:4b
ollama run deepseek-r1:7b
ollama run qwen3:4b
```

2、安装 python 和 pycharm 后，安装依赖

```
pip install -r requirements
```

3、修改为本地的大模型名字

```python
# 初始化Ollama模型
ollama = Ollama(
    model_name=os.getenv('OLLAMA_MODEL', 'deepseek-r1:7b'),
    base_url=os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434'),  # 默认本地地址
    prompt="你是一个小助理，用简洁利落的话回复人们的各种问题，回答时以txt纯文字格式，不要啰啰嗦嗦，风格像人在微信聊天那样，不要像个人机。在回复时，请将你的思考过程放在<think>和</think>标签之间，只将最终回复内容放在标签之外。"
)
```

4、修改监听目标，也就是要关注哪几个微信用户的信息并回复他们

```python
# 指定监听目标
listen_list = [
    '小号',
    '文件传输助手'
]
```

5、打开微信桌面版并登录，保持打开状态。然后打开 ollama_chat.py，运行程序

### 测试:

​	prompt="你是一个微信小助理，用简洁利落的话回复人们的各种问题，回答时以txt纯文字格式，不要啰啰嗦嗦，风格像人在微信聊天那样，不要像个人机。在回复时，请将你的思考过程放在 <think> 和 </think> 标签之间，只将最终回复内容放在标签之外。"

#### 1、测试问题：

- “杭州最出名的是什么”

- “昨天你推荐的咖啡点真不错！不过你之前说的要养猫，决定要养了吗”

- “真的烦。。我挺想离职的。。。”

- [动画表情]

<img src="./md-pictures/吸吮手指-微信动画表情.jpg#pi" alt="动画表情" style="zoom:50%;" />

​	程序启动后，仅测试当前单个微信用户的对话，测试调用下一个模型时，重新启动ollama重置大模型。
#### 2、测试结果（仅供参考）

​	测了两轮而已，前一轮只有1 2 3问题，第二轮有包括表情包的全部共4个问题。

**测试结果截图**：

<img src="./md-pictures/deepseek-r17b对话.png#pi" alt="动画表情" style="zoom:60%;" />

<img src="./md-pictures/qwen3对话.png#pi" alt="动画表情" style="zoom:60%;" />

<img src="./md-pictures/gemma3对话.png#pi" alt="动画表情" style="zoom:60%;" />

**测量指标**：

- **平均回复用时**：假设用户（小号）的预期对方（我）打字回复时间耐心值为10秒。

- **回复质量**：第一是从用户视角关注的，我要我觉得不要你觉得，用户主观感受至上；第二是回答的内容相关性，能和用户聊到一个话题。

- **综合评分**：用时和质量各占一半。

| 大模型名称 | 平均回复用时（单位：秒） | 回复质量（0.6 * 自我感觉 + 0.4 * 相关性，满分10分） | 综合评分（0.5 * 100/用时 + 0.5 * 质量，满分10分） |
| :--------: | :----: | :--------: | :--------: |
| deepseek-r1:7b     | (14+15+11+25+17+12+17)/7 =15 | 0.6 * 8 + 0.4 * 8=8 | 0.5 *100/15 + 0.5 * 8=7.3 |
| qwen3:4b     | (23+13+14+25+15+17+13)/7 =17 | 0.6 * 9+ 0.4 * 9 =9 | 0.5 * 100/17 + 0.5 * 9=7.4 |
| gemma3:4b | (14+12+9+13+11+9+9)/7 =11 | 0.6 * 10+ 0.4 * 9.5=9.8 | 0.5 *100/11 + 0.5 * 9.8=9.4 |

​	因此 9.4>7.4>7.3，可见 gemma3 用在微信机器人回复消息的功能上，体验还是很好的。



### 感受

​	还得是美国佬做的大模型质量好，毕竟理论知识都是他们搞的，大模型高质量的资料都是英文，而且清晰实在，而国内可能就用到“防火墙”的信息阻隔去卖课卖资料，国内做应用的多，但各有所长吧，美国那边适合发展 AI 基础生态，中国适合将技术落地做应用。

​	扯远了。deepseek 由于不支持图像，所以实在可惜，识别不了表情包。而通义千问 qwen3 还是 4 月底新出的，参数和往常一样堆得很猛，但是使用下来的体验就不那么尽人意，特别是每次启动问第一个问题花了近两倍时间才回答，是不是 qwen3 有冷启动？反而 gemma3 越问回答越快，连续的回答的用时还算平缓的减少，这就特别像人在忙，看到微信来消息拿手机打字回复，聊嗨了就回复快起来，想象一下吧，这是很贴合实际人在微信聊天的情况，这也是为什么我打 10 分自我感觉分，因为最贴合我想要的：“你是一个人，你在用微信聊天回消息”。

​	这第一版本缺陷很多，比如不同用户之间的信息会互相污染，比如用户1和微信机器人聊天，用户2问微信机器人之前和聊过什么，就泄露了和用户1的对话隐私，应要实现每个微信用户有各自的独立对话会话。还有多次测试时大模型会宕机，也许后续需要加上一个机制，一个大模型生成的内容是明显不符合要求的，就立刻调用备用大模型生成。确保完全生成好完整的回复内容，再进行切割处理发送消息。

​	后续再继续完成第2版吧，打算加上数据库和 MCP 工具，将聊天记录存储到mysql那样，优化检索，维持长期合作的客户，MCP 接入多种工具像地图搜索之类。做成定制化的 AI Agent。而不是只能在像 coze 的平台捆绑住，coze 的确是想要快速验证它的 AI 产品，快速迭代，但是 AI Agent 绝不只限于平台上。



